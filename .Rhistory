theme_minimal() +
theme(axis.text.y = element_text(size = 8))
bigram_freq <- empire_texts %>%
filter(author == "Haggard, H. Rider (Henry Rider)") %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
# Remove rows with NA bigrams
filter(!is.na(bigram)) %>%
# Separate bigrams into two words to filter stop words
separate(bigram, into = c("yellow", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word,
!word2 %in% stop_words$word) %>%
# Recombine into bigrams and clean
unite(bigram, word1, word2, sep = " ") %>%
# Count bigram frequencies
count(bigram, sort = TRUE) %>%
top_n(10, n)
# Second analysis
# Filter for a specific author and tokenize into bigrams
bigram_freq <- empire_texts %>%
filter(author == "Haggard, H. Rider (Henry Rider)") %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
# Remove rows with NA bigrams
filter(!is.na(bigram)) %>%
# Separate bigrams into two words to filter stop words
separate(bigram, into = c(yellow, "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word,
!word2 %in% stop_words$word) %>%
# Recombine into bigrams and clean
unite(bigram, word1, word2, sep = " ") %>%
# Count bigram frequencies
count(bigram, sort = TRUE) %>%
top_n(10, n)
# Second analysis
# Filter for a specific author and tokenize into bigrams
bigram_freq <- empire_texts %>%
filter(author == "Haggard, H. Rider (Henry Rider)") %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
# Remove rows with NA bigrams
filter(!is.na(bigram)) %>%
# Separate bigrams into two words to filter stop words
separate(bigram, into = c("word1"), "word2"), sep = " ") %>%
# Second analysis
# Filter for a specific author and tokenize into bigrams
bigram_freq <- empire_texts %>%
filter(author == "Haggard, H. Rider (Henry Rider)") %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
# Remove rows with NA bigrams
filter(!is.na(bigram)) %>%
# Separate bigrams into two words to filter stop words
separate(bigram, into = c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word,
!word2 %in% stop_words$word) %>%
# Recombine into bigrams and clean
unite(bigram, word1, word2, sep = " ") %>%
# Count bigram frequencies
count(bigram, sort = TRUE) %>%
top_n(10, n)
# Create bar plot
ggplot(bigram_freq, aes(x = reorder(bigram, n), y = n)) +
geom_bar(stat = "identity", fill = "darkorange") +
coord_flip() +
labs(title = "Top 10 Bigrams by H. Rider Haggard",
x = "Bigrams", y = "Frequency") +
theme_minimal() +
theme(axis.text.y = element_text(size = 8))
bigram_freq <- empire_texts %>%
filter(author == "Haggard, H. Rider (Henry Rider)") %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
filter(!is.na(bigram)) %>%
separate(bigram, into = c("word1", "word2"), sep = " ") %>%
# Filter for bigrams where word2 is "land"
filter(word2 == "land") %>%
filter(!word1 %in% stop_words$word) %>%
unite(bigram, word1, word2, sep = " ") %>%
count(bigram, sort = TRUE) %>%
top_n(10, n)
ggplot(bigram_freq, aes(x = reorder(bigram, n), y = n)) +
geom_bar(stat = "identity", fill = "darkorange") +
coord_flip() +
labs(title = "Top 10 Bigrams by H. Rider Haggard",
x = "Bigrams", y = "Frequency") +
theme_minimal() +
theme(axis.text.y = element_text(size = 8))
set.seed(279)
sample_titles <- empire_texts %>%
distinct(title) %>%
slice_sample(n = 4) %>%
pull(title)
# Filter, tokenize, and calculate TF-IDF
tfidf_data <- empire_texts %>%
filter(title %in% sample_titles) %>%
unnest_tokens(word, text) %>%
anti_join(stop_words, by = "word") %>%
count(title, word, name = "n") %>%
bind_tf_idf(word, title, n) %>%
# Get top 5 words per title by TF-IDF
group_by(title) %>%
slice_max(order_by = tf_idf, n = 5, with_ties = FALSE) %>%
ungroup()
# Create faceted bar plot
ggplot(tfidf_data, aes(x = reorder_within(word, tf_idf, title), y = tf_idf)) +
geom_bar(stat = "identity", fill = "purple") +
facet_wrap(~title, scales = "free_y") +
coord_flip() +
scale_x_reordered() +
labs(title = "Top 5 Distinctive Words by Title (TF-IDF)",
x = "Words", y = "TF-IDF Score") +
theme_minimal() +
theme(axis.text.y = element_text(size = 6))
packages <- c("httr", "jsonlite", "dplyr", "tidytext", "ggplot2", "stringr", "lubridate")
new_packages <- packages[!packages %in% installed.packages()[,"Package"]]
if(length(new_packages)) install.packages(new_packages)
library(httr)
library(jsonlite)
library(dplyr)
library(tidytext)
library(ggplot2)
library(stringr)
library(lubridate)
url <- "https://bsky.social/xrpc/com.atproto.server.createSession"
body <- list(
identifier = identifier,
password = Steward1173
)
# Function to authenticate with Bluesky
bluesky_auth <- function(identifier, password) {
url <- "https://bsky.social/xrpc/com.atproto.server.createSession"
body <- list(
identifier = bkatzir,
password = Steward1173
)
response <- POST(
url = url,
body = body,
encode = "json"
)
if (status_code(response) == 200) {
content <- content(response, "parsed")
return(list(
accessJwt = content$accessJwt,
refreshJwt = content$refreshJwt,
did = content$did
))
} else {
stop("Authentication failed. Check your credentials.")
}
}
# Function to search for posts
search_posts <- function(auth, query, limit = 100) {
url <- paste0("https://bsky.social/xrpc/app.bsky.feed.searchPosts?q=", URLencode(query), "&limit=", limit)
response <- GET(
url = url,
add_headers(Authorization = paste("Bearer", auth$accessJwt))
)
if (status_code(response) == 200) {
content <- content(response, "parsed")
return(content$posts)
} else {
stop("Failed to search posts. Status code: ", status_code(response))
}
}
# Function to authenticate with Bluesky
bluesky_auth <- function(identifier, password) {
url <- "https://bsky.social/xrpc/com.atproto.server.createSession"
body <- list(
identifier = bkatzir,
password = Steward1173
)
response <- POST(
url = url,
body = body,
encode = "json"
)
if (status_code(response) == 200) {
content <- content(response, "parsed")
return(list(
accessJwt = content$accessJwt,
refreshJwt = content$refreshJwt,
did = content$did
))
} else {
stop("Authentication failed. Check your credentials.")
}
}
# Function to search for posts
search_posts <- function(auth, query, limit = 100) {
url <- paste0("https://bsky.social/xrpc/app.bsky.feed.searchPosts?q=", URLencode(query), "&limit=", limit)
response <- GET(
url = url,
add_headers(Authorization = paste("Bearer", auth$accessJwt))
)
if (status_code(response) == 200) {
content <- content(response, "parsed")
return(content$posts)
} else {
stop("Failed to search posts. Status code: ", status_code(response))
}
}
# Function to get post details by reference
get_post_thread <- function(auth, uri, depth = 5) {
url <- paste0("https://bsky.social/xrpc/app.bsky.feed.getPostThread?uri=", URLencode(uri), "&depth=", depth)
response <- GET(
url = url,
add_headers(Authorization = paste("Bearer", auth$accessJwt))
)
if (status_code(response) == 200) {
content <- content(response, "parsed")
return(content$thread)
} else {
stop("Failed to get post thread. Status code: ", status_code(response))
}
}
# Function to extract text from posts
extract_post_data <- function(posts) {
# Creating a data frame to store post data
post_data <- data.frame(
uri = character(),
cid = character(),
author = character(),
text = character(),
timestamp = character(),
likes = integer(),
reposts = integer(),
stringsAsFactors = FALSE
)
# Extracting data from each post
for (post in posts) {
if (!is.null(post$record)) {
new_row <- data.frame(
uri = post$uri,
cid = post$cid,
author = post$author$handle,
text = post$record$text,
timestamp = post$indexedAt,
likes = ifelse(is.null(post$likeCount), 0, post$likeCount),
reposts = ifelse(is.null(post$repostCount), 0, post$repostCount),
stringsAsFactors = FALSE
)
post_data <- rbind(post_data, new_row)
}
}
# Convert timestamp to proper datetime format
post_data$timestamp <- as.POSIXct(post_data$timestamp, format = "%Y-%m-%dT%H:%M:%S", tz = "UTC")
return(post_data)
}
# Function to analyze antisemitic content
analyze_antisemitism <- function(post_data) {
# Define antisemitic terms and dog whistles from across the political spectrum
# Create separate categorized lists for better analysis
# Far-right antisemitic terminology
far_right_terms <- c(
"jew banker", "zionist conspiracy", "globalist", "rothschild",
"soros", "jq", "jewish question", "zog", "holohoax",
"goyim", "international jewry", "protocols of zion",
"cultural marxism", "jew world order", "jewish world order",
"blood libel", "kike", "shekel", "nose", "happy merchant",
"six gorillion", "holocaust denial", "judeo-bolshevism"
)
# Far-left antisemitic terminology (often disguised as anti-Israel/anti-Zionist)
far_left_terms <- c(
"zionazi", "israelis are nazis", "zios", "jewish lobby",
"aipac controls", "israel controls", "israel money",
"jewish influence", "zionist influence", "zionist occupied",
"apartheid state", "settler colonial", "ethnic cleansing",
"genocide israel", "israel genocide", "israel war crimes",
"israel human rights", "divest israel", "boycott israel",
"from the river to the sea", "free palestine genocide",
"colonizer jews", "white jews", "european jews", "fake jews",
"ashkenazi not real", "khazar", "zionist entity"
)
# Context-dependent terminology (requires additional context analysis)
# These terms are not inherently antisemitic but may be used in antisemitic contexts
context_dependent_terms <- c(
"dual loyalty", "cosmopolitan", "rootless", "globalist elite",
"international bankers", "new world order", "cabal", "elite",
"masters of", "puppet masters", "pulling strings", "controlling media",
"liberal elites", "george soros", "rothschild", "goldman sachs"
)
# Anti-Israel terminology that might be legitimate criticism or might be antisemitic
# depending on context, framing, and application
anti_israel_terms <- c(
"israeli occupation", "israeli settlements", "israeli apartheid",
"israeli colonialism", "israeli war crimes", "israeli human rights",
"bds movement", "boycott israel", "divest israel", "sanction israel",
"free palestine", "end occupation", "no aid to israel",
"israeli aggression", "israeli violence", "israeli brutality"
)
# Combine all terms for initial detection
all_terms <- c(far_right_terms, far_left_terms, context_dependent_terms, anti_israel_terms)
pattern <- paste(all_terms, collapse = "|")
# Identify posts containing potentially antisemitic terms
post_data$contains_antisemitism <- str_detect(
tolower(post_data$text),
pattern
)
# Add more detailed categorization
post_data$far_right_antisemitism <- str_detect(
tolower(post_data$text),
paste(far_right_terms, collapse = "|")
)
post_data$far_left_antisemitism <- str_detect(
tolower(post_data$text),
paste(far_left_terms, collapse = "|")
)
post_data$context_dependent <- str_detect(
tolower(post_data$text),
paste(context_dependent_terms, collapse = "|")
)
post_data$anti_israel <- str_detect(
tolower(post_data$text),
paste(anti_israel_terms, collapse = "|")
)
# Calculate sentiment for posts
post_data$sentiment <- sentiment_score(post_data$text)
return(post_data)
}
# Function for more nuanced analysis of context-dependent terms
analyze_context <- function(post_data) {
# This function would implement more sophisticated NLP techniques
# to analyze the context in which potentially antisemitic terms are used
# For posts containing context-dependent terms
context_posts <- post_data %>%
filter(context_dependent == TRUE)
# Example: Check if these posts contain other antisemitic markers
context_posts$likely_antisemitic <- (
context_posts$far_right_antisemitism |
context_posts$far_left_antisemitism |
(context_posts$sentiment < -2)  # Strongly negative sentiment as a proxy
)
# Update the main dataset
post_data$likely_antisemitic <- FALSE
post_data$likely_antisemitic[post_data$far_right_antisemitism | post_data$far_left_antisemitism] <- TRUE
# Update for context-dependent posts
context_indices <- which(post_data$context_dependent)
context_post_indices <- match(context_posts$uri, post_data$uri[context_indices])
post_data$likely_antisemitic[context_indices[context_post_indices]] <- context_posts$likely_antisemitic
return(post_data)
}
# Simple sentiment analysis function
sentiment_score <- function(texts) {
# Load sentiment lexicon
data("sentiments")
afinn <- get_sentiments("afinn")
# Tokenize text
words <- texts %>%
tibble(text = .) %>%
unnest_tokens(word, text)
# Calculate sentiment
sentiment_scores <- words %>%
inner_join(afinn, by = "word") %>%
group_by(text) %>%
summarize(sentiment = sum(value, na.rm = TRUE))
# Merge back with original texts
result <- tibble(text = texts) %>%
left_join(sentiment_scores, by = "text") %>%
mutate(sentiment = replace_na(sentiment, 0))
return(result$sentiment)
}
# Visualize antisemitism by political spectrum
visualize_antisemitism_spectrum <- function(post_data) {
# Prepare data for visualization
spectrum_data <- data.frame(
category = c("Far-right", "Far-left", "Context-dependent", "Anti-Israel/Zionist"),
count = c(
sum(post_data$far_right_antisemitism, na.rm = TRUE),
sum(post_data$far_left_antisemitism, na.rm = TRUE),
sum(post_data$context_dependent, na.rm = TRUE),
sum(post_data$anti_israel, na.rm = TRUE)
)
)
# Calculate percentages
total_posts <- nrow(post_data)
spectrum_data$percentage <- (spectrum_data$count / total_posts) * 100
# Create bar plot
ggplot(spectrum_data, aes(x = reorder(category, -percentage), y = percentage)) +
geom_bar(stat = "identity", fill = "steelblue") +
theme_minimal() +
labs(
title = "Distribution of Potentially Antisemitic Content by Category",
x = "Category",
y = "Percentage of Total Posts (%)",
caption = "Source: Analysis of Bluesky data"
) +
coord_flip()
}
# Function to authenticate with Bluesky
bluesky_auth <- function(identifier, password) {
url <- "https://bsky.social/xrpc/com.atproto.server.createSession"
body <- list(
identifier = bkatzir,
password = Steward1173
)
response <- POST(
url = url,
body = body,
encode = "json"
)
if (status_code(response) == 200) {
content <- content(response, "parsed")
return(list(
accessJwt = content$accessJwt,
refreshJwt = content$refreshJwt,
did = content$did
))
} else {
stop("Authentication failed. Check your credentials.")
}
}
setwd("~/GitHub/analytics")
install.packages(c("ggplot2", "dplyr", "tidyr", "scales", "gridExtra", "RColorBrewer"))
library(ggplot2)
library(dplyr)
library(tidyr)
library(scales)
library(gridExtra)
library(RColorBrewer)
# Read the CSV file
df <- read.csv('libguides-stats - guide-stats.csv')
# Define months
months <- c('X2024.06', 'X2024.07', 'X2024.08', 'X2024.09', 'X2024.10', 'X2024.11',
'X2024.12', 'X2025.01', 'X2025.02', 'X2025.03', 'X2025.04', 'X2025.05', 'X2025.06')
# 1. Top 10 Guides by Total Usage
top_10 <- df %>%
arrange(desc(Total)) %>%
head(10) %>%
mutate(Guide.Name = ifelse(nchar(Guide.Name) > 30,
paste0(substr(Guide.Name, 1, 30), "..."),
Guide.Name))
p1 <- ggplot(top_10, aes(x = reorder(Guide.Name, Total), y = Total)) +
geom_col(fill = "steelblue", alpha = 0.8) +
coord_flip() +
labs(title = "Top 10 LibGuides by Total Usage",
x = "Guide Name",
y = "Total Views") +
theme_minimal() +
theme(axis.text.y = element_text(size = 8))
shiny::runApp('~/analytics')
runApp('~/analytics')
runApp('~/analytics')
runApp('~/analytics')
install.packages(c("shiny", "shinydashboard", "DT", "ggplot2", "dplyr", "tidyr", "plotly", "RColorBrewer"))
runApp('~/analytics')
install.packages(c("shiny", "shinydashboard", "DT", "ggplot2", "dplyr", "tidyr", "plotly", "RColorBrewer"))
runApp('~/analytics')
shiny::runApp()
ls
pwd
runApp()
runApp()
runApp()
runApp()
install.packages(c("shiny", "shinydashboard", "DT", "ggplot2", "dplyr", "tidyr", "plotly", "RColorBrewer"))
runApp()
shiny::runApp()
library(shiny); runApp('libguides-module.R')
install.packages(c("shiny", "shinydashboard", "DT", "ggplot2", "dplyr", "tidyr", "plotly", "RColorBrewer"))
runApp()
shiny::runApp()
shiny::runApp()
checkwd()
setwd("~/GitHub/analytics/modules")
shiny::runApp('~/GitHub/analytics')
runApp('~/GitHub/analytics')
getwd()
cd ..
setwd("~/GitHub/analytics")
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
runApp('app.R')
library(shiny); runApp('app.R')
install.packages(c("shiny", "shinydashboard", "DT", "ggplot2", "dplyr", "tidyr", "plotly", "RColorBrewer"))
runApp('app.R')
setwd("~/GitHub/analytics")
runApp('app.R')
runApp('app.R')
runApp('app.R')
runApp()
runApp()
runApp()
runApp()
install.packages(c("ggplot2", "dplyr", "tidyr", "scales", "gridExtra", "RColorBrewer"))
runApp()
source("modules/libguides_module.R")
install.packages(c("ggplot2", "dplyr", "tidyr", "scales", "gridExtra", "RColorBrewer"))
shiny::runApp()
install.packages(c("ggplot2", "dplyr", "tidyr", "scales", "gridExtra", "RColorBrewer"))
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
# Download handlers
output$download_csv <- downloadHandler(
filename = function() {
paste("libguides_data_", Sys.Date(), ".csv", sep = "")
},
content = function(file) {
write.csv(filtered_data(), file, row.names = FALSE)
}
)
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
